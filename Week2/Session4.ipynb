{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Techniques with classifiers\n",
    "\n",
    "In this session, we'll play around with classifiers, and techniques to optimize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Imports and load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData= [[ -1.54609786e+00  -1.34499549e+00   2.75868709e-16   4.81287772e-01\n",
      "   -4.44999502e-01   1.27192065e+00  -2.40989649e+00   1.28580768e+00\n",
      "    5.98139336e-01]\n",
      " [ -3.52090723e-01  -1.34499549e+00   9.22992157e-03  -4.79086761e-01\n",
      "   -4.44999502e-01  -4.42748897e-01   5.07835440e-01   1.23087575e+00\n",
      "   -7.19428002e-01]\n",
      " [ -1.54609786e+00  -1.34499549e+00  -4.56670909e-01   2.40203684e+00\n",
      "    1.86652569e+00   1.27192065e+00  -1.92360784e+00  -6.68199626e-01\n",
      "   -7.19428002e-01]\n",
      " [  8.41916418e-01   7.43496915e-01   2.75868709e-16   4.81287772e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01  -8.35669181e-02\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01  -1.34499549e+00  -1.00022188e+00  -4.79086761e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01  -4.87708992e-01\n",
      "   -7.19428002e-01]\n",
      " [ -3.52090723e-01   7.43496915e-01  -2.23720494e-01   4.81287772e-01\n",
      "   -4.44999502e-01   4.14585875e-01   5.07835440e-01   1.58008589e+00\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01   7.43496915e-01  -3.01370632e-01  -4.79086761e-01\n",
      "   -4.44999502e-01  -4.42748897e-01   5.07835440e-01  -1.64912699e+00\n",
      "    2.68747501e-01]\n",
      " [ -3.52090723e-01  -1.34499549e+00  -2.16497396e+00   4.81287772e-01\n",
      "    7.10763093e-01   4.14585875e-01   5.07835440e-01   8.81665607e-01\n",
      "   -7.19428002e-01]\n",
      " [  8.41916418e-01   7.43496915e-01  -5.34321048e-01  -4.79086761e-01\n",
      "   -4.44999502e-01  -4.42748897e-01   5.07835440e-01   6.42319330e-01\n",
      "    2.68747501e-01]\n",
      " [ -3.52090723e-01   7.43496915e-01  -1.00022188e+00  -4.79086761e-01\n",
      "   -4.44999502e-01   1.27192065e+00   5.07835440e-01  -8.99698484e-01\n",
      "    2.68747501e-01]]  with shape  (654, 9)\n",
      "TrainLables= id\n",
      "277     1\n",
      "562     1\n",
      "111     1\n",
      "930     0\n",
      "841     0\n",
      "585     0\n",
      "609     0\n",
      "540     1\n",
      "1075    0\n",
      "390     0\n",
      "Name: survived, dtype: int64 (654,)\n",
      "TestData= [[  8.41916418e-01  -1.34499549e+00  -8.44921602e-01   4.81287772e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01  -1.60204248e+00\n",
      "   -7.19428002e-01]\n",
      " [  8.41916418e-01  -1.34499549e+00   2.75868709e-16   4.81287772e-01\n",
      "   -4.44999502e-01   4.14585875e-01   5.07835440e-01  -9.03622193e-01\n",
      "    5.98139336e-01]\n",
      " [ -3.52090723e-01  -1.34499549e+00  -2.00967368e+00   1.44166230e+00\n",
      "    7.10763093e-01   1.27192065e+00  -4.64741870e-01  -1.41762813e+00\n",
      "   -7.19428002e-01]\n",
      " [ -1.54609786e+00   7.43496915e-01   8.68800601e-02   4.81287772e-01\n",
      "   -4.44999502e-01   1.27192065e+00  -2.40989649e+00  -8.64385099e-01\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01   7.43496915e-01   2.42180337e-01  -4.79086761e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01   2.53872095e-01\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01   7.43496915e-01  -6.84202169e-02  -4.79086761e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01  -9.38935578e-01\n",
      "    2.68747501e-01]\n",
      " [ -1.54609786e+00   7.43496915e-01  -3.79020771e-01   4.81287772e-01\n",
      "   -4.44999502e-01   1.27192065e+00  -9.51030525e-01  -4.52395606e-01\n",
      "    2.68747501e-01]\n",
      " [ -1.54609786e+00   7.43496915e-01   2.75868709e-16  -4.79086761e-01\n",
      "   -4.44999502e-01   4.14585875e-01   5.07835440e-01  -2.79752391e-01\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01   7.43496915e-01   1.64530199e-01  -4.79086761e-01\n",
      "   -4.44999502e-01  -1.30008367e+00   5.07835440e-01   7.48259486e-01\n",
      "    2.68747501e-01]\n",
      " [  8.41916418e-01  -1.34499549e+00  -7.67271463e-01  -4.79086761e-01\n",
      "   -4.44999502e-01  -4.42748897e-01   5.07835440e-01   6.69785297e-01\n",
      "   -7.19428002e-01]]  with shape  (655, 9)\n",
      "TestLabels= id\n",
      "621    NaN\n",
      "757    NaN\n",
      "340    NaN\n",
      "91     NaN\n",
      "996    NaN\n",
      "746    NaN\n",
      "143    NaN\n",
      "166    NaN\n",
      "1110   NaN\n",
      "1091   NaN\n",
      "Name: survived, dtype: float64 (654,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add input as import path\n",
    "sys.path.insert(0,'../input')\n",
    "\n",
    "import joblib #or your dataset handler\n",
    "X, Y = joblib.load(\"traindata.pkl\")\n",
    "X_test, Y_test = joblib.load(\"testdata.pkl\")\n",
    "\n",
    "#Double check if all is well\n",
    "print(\"TrainData=\",X[0:10],\" with shape \", X.shape)\n",
    "print(\"TrainLables=\", Y[0:10], Y.shape) \n",
    "\n",
    "print(\"TestData=\",X_test[0:10], \" with shape \",X_test.shape)\n",
    "print(\"TestLabels=\", Y_test[0:10], Y.shape) # We don't have test labels. Should be NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. HyperParameters\n",
    "\n",
    "HyperParameters are just high level parameters which affect the training of a given classifier. \n",
    "_If you have implemented the datasetloader you could introduce feature variables into a parameter search too_\n",
    "\n",
    "Here, we experiment with SVM classifier. You need to try more\n",
    "\n",
    "* Q1. Experiment with the RandomForests\n",
    "* Q2. Increase search space\n",
    "* Q3. Use the library: __hyperopt__ and do more things\n",
    "    * Hint : [http://hyperopt.github.io/hyperopt/]\n",
    "\n",
    "scikit-learn module here  : __sklearn.model_selection__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.300 (+/-0.001) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.300 (+/-0.001) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.740 (+/-0.073) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.300 (+/-0.001) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.757 (+/-0.061) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.738 (+/-0.079) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.766 (+/-0.060) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.752 (+/-0.064) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.765 (+/-0.070) for {'C': 1, 'kernel': 'linear'}\n",
      "0.755 (+/-0.044) for {'C': 10, 'kernel': 'linear'}\n",
      "0.755 (+/-0.044) for {'C': 100, 'kernel': 'linear'}\n",
      "0.755 (+/-0.044) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.86      0.86       204\n",
      "          1       0.77      0.77      0.77       123\n",
      "\n",
      "avg / total       0.83      0.83      0.83       327\n",
      "\n",
      "Accuracy :  0.8256880733944955\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.726 (+/-0.066) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.500 (+/-0.000) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.060) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.724 (+/-0.070) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.755 (+/-0.057) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.736 (+/-0.056) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.753 (+/-0.058) for {'C': 1, 'kernel': 'linear'}\n",
      "0.743 (+/-0.032) for {'C': 10, 'kernel': 'linear'}\n",
      "0.743 (+/-0.032) for {'C': 100, 'kernel': 'linear'}\n",
      "0.743 (+/-0.032) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.86      0.86      0.86       204\n",
      "          1       0.77      0.77      0.77       123\n",
      "\n",
      "avg / total       0.83      0.83      0.83       327\n",
      "\n",
      "Accuracy :  0.8256880733944955\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "from utils import accuracy_score_numpy\n",
    "\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "parameters_to_tune = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],\n",
    "                     'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "\n",
    "# Since we don't really have labelled test data, we will split our training data into a new test and train\n",
    "#####!!!! Notice the caps - Mayasculo Menusculo\n",
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.5, random_state=0)\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clfgrid = GridSearchCV(SVC(), parameters_to_tune, cv=5,\n",
    "                       scoring='%s_macro' % score)\n",
    "    clfgrid.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clfgrid.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clfgrid.cv_results_['mean_test_score']\n",
    "    stds = clfgrid.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clfgrid.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clfgrid.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Accuracy : \", np.count_nonzero(y_true==y_pred)/len(y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1790.4038321743124, 'class_weight': 'balanced', 'gamma': 0.029690219996218577, 'kernel': 'linear'}\n",
      "\n",
      "Random scores on development set:\n",
      "\n",
      "0.771 (+/-0.082) for {'C': 2875.1204361646783, 'class_weight': 'balanced', 'gamma': 0.070024172813174995, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 569.00502651030354, 'class_weight': None, 'gamma': 0.03381156095995333, 'kernel': 'linear'}\n",
      "0.667 (+/-0.061) for {'C': 1059.733773577402, 'class_weight': 'balanced', 'gamma': 0.16400436046777886, 'kernel': 'rbf'}\n",
      "0.679 (+/-0.018) for {'C': 1396.5177333418828, 'class_weight': 'balanced', 'gamma': 0.057010758820175339, 'kernel': 'rbf'}\n",
      "0.679 (+/-0.049) for {'C': 4056.1042518863442, 'class_weight': None, 'gamma': 0.072668920684137453, 'kernel': 'rbf'}\n",
      "0.670 (+/-0.092) for {'C': 1973.1937577691358, 'class_weight': 'balanced', 'gamma': 0.28874897795011328, 'kernel': 'rbf'}\n",
      "0.664 (+/-0.085) for {'C': 924.35567841150839, 'class_weight': 'balanced', 'gamma': 0.23866798846764051, 'kernel': 'rbf'}\n",
      "0.685 (+/-0.021) for {'C': 621.72606624923787, 'class_weight': None, 'gamma': 0.090046381519944776, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.029) for {'C': 1221.501263294045, 'class_weight': None, 'gamma': 0.058709931665534822, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 1655.9618519029177, 'class_weight': None, 'gamma': 0.04633274190156253, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 1532.2267100823672, 'class_weight': None, 'gamma': 0.28367960303281364, 'kernel': 'linear'}\n",
      "0.673 (+/-0.026) for {'C': 960.44898437480617, 'class_weight': None, 'gamma': 0.094502250705089649, 'kernel': 'rbf'}\n",
      "0.771 (+/-0.082) for {'C': 223.15064653751088, 'class_weight': 'balanced', 'gamma': 0.0039216746628938847, 'kernel': 'linear'}\n",
      "0.682 (+/-0.030) for {'C': 1208.965519954492, 'class_weight': 'balanced', 'gamma': 0.052562203751182292, 'kernel': 'rbf'}\n",
      "0.771 (+/-0.082) for {'C': 237.85536143279742, 'class_weight': 'balanced', 'gamma': 0.033055271083593421, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 3158.2634705935816, 'class_weight': None, 'gamma': 0.14084940580521949, 'kernel': 'linear'}\n",
      "0.673 (+/-0.055) for {'C': 304.87551743213641, 'class_weight': None, 'gamma': 0.22897259509856074, 'kernel': 'rbf'}\n",
      "0.691 (+/-0.033) for {'C': 1478.4041696071072, 'class_weight': 'balanced', 'gamma': 0.074943906147219977, 'kernel': 'rbf'}\n",
      "0.685 (+/-0.022) for {'C': 774.89304965668612, 'class_weight': 'balanced', 'gamma': 0.057765280298142423, 'kernel': 'rbf'}\n",
      "0.703 (+/-0.020) for {'C': 375.64610937175206, 'class_weight': 'balanced', 'gamma': 0.029838160465286513, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.80      0.84       204\n",
      "          1       0.72      0.82      0.77       123\n",
      "\n",
      "avg / total       0.82      0.81      0.81       327\n",
      "\n",
      "Accuracy :  0.8103975535168195\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1790.4038321743124, 'class_weight': 'balanced', 'gamma': 0.029690219996218577, 'kernel': 'linear'}\n",
      "\n",
      "Random scores on development set:\n",
      "\n",
      "0.734 (+/-0.026) for {'C': 746.22697154085392, 'class_weight': 'balanced', 'gamma': 0.0059510096516223005, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.029) for {'C': 128.09125570423808, 'class_weight': None, 'gamma': 0.29090830062093714, 'kernel': 'linear'}\n",
      "0.771 (+/-0.082) for {'C': 1560.7126520360323, 'class_weight': 'balanced', 'gamma': 0.11759013763702152, 'kernel': 'linear'}\n",
      "0.679 (+/-0.026) for {'C': 1442.8900025786309, 'class_weight': 'balanced', 'gamma': 0.050708903417604957, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.029) for {'C': 20.347068838421759, 'class_weight': None, 'gamma': 0.0076288717438735268, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 607.96598789464508, 'class_weight': None, 'gamma': 0.10966327592980477, 'kernel': 'linear'}\n",
      "0.688 (+/-0.022) for {'C': 2030.2608790738539, 'class_weight': None, 'gamma': 0.012765664117635151, 'kernel': 'rbf'}\n",
      "0.771 (+/-0.082) for {'C': 380.31831244997846, 'class_weight': 'balanced', 'gamma': 0.11479193514635125, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 691.51735921462762, 'class_weight': None, 'gamma': 0.18925884265319354, 'kernel': 'linear'}\n",
      "0.670 (+/-0.101) for {'C': 354.51676435924844, 'class_weight': 'balanced', 'gamma': 0.31022020045477877, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.029) for {'C': 366.96206446111449, 'class_weight': None, 'gamma': 0.32483659207080273, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 280.11975001906438, 'class_weight': None, 'gamma': 0.028987915661338227, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 667.25009438053269, 'class_weight': None, 'gamma': 0.012192646595671038, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 2182.6506056709472, 'class_weight': None, 'gamma': 0.037291848473714251, 'kernel': 'linear'}\n",
      "0.679 (+/-0.005) for {'C': 3461.9117961913485, 'class_weight': None, 'gamma': 0.049672263860770921, 'kernel': 'rbf'}\n",
      "0.725 (+/-0.036) for {'C': 535.52287035507004, 'class_weight': 'balanced', 'gamma': 0.010980778193864581, 'kernel': 'rbf'}\n",
      "0.743 (+/-0.029) for {'C': 397.65060801296568, 'class_weight': None, 'gamma': 0.048321542049755209, 'kernel': 'linear'}\n",
      "0.771 (+/-0.082) for {'C': 756.54726593060263, 'class_weight': 'balanced', 'gamma': 0.20018650138452548, 'kernel': 'linear'}\n",
      "0.771 (+/-0.082) for {'C': 1411.099287127975, 'class_weight': 'balanced', 'gamma': 0.2432374765461619, 'kernel': 'linear'}\n",
      "0.743 (+/-0.029) for {'C': 147.85452972277815, 'class_weight': None, 'gamma': 0.020583450638055778, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.88      0.80      0.84       204\n",
      "          1       0.72      0.82      0.77       123\n",
      "\n",
      "avg / total       0.82      0.81      0.81       327\n",
      "\n",
      "Accuracy :  0.8103975535168195\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import scipy\n",
    "\n",
    "#And now, let's try some Random searches instead,\n",
    "param_space = {'C': scipy.stats.expon(scale=1000), 'gamma': scipy.stats.expon(scale=.1),\n",
    "  'kernel': ['rbf','linear'], 'class_weight':['balanced', None]}\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "n_iter_search = 20\n",
    "clf_to_optimize = SVC()\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "    \n",
    "    clfrnd = RandomizedSearchCV(clf_to_optimize, param_distributions=param_space,\n",
    "                                   n_iter=n_iter_search)\n",
    "    clfrnd.fit(x_train, y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Random scores on development set:\")\n",
    "    print()\n",
    "    means = clfrnd.cv_results_['mean_test_score']\n",
    "    stds = clfrnd.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clfrnd.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, clfrnd.predict(x_test)\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print(\"Accuracy : \", np.count_nonzero(y_true==y_pred)/len(y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Grid - 0.798473282443\n",
      "Accuracy RandomSearch - 0.772519083969\n"
     ]
    }
   ],
   "source": [
    "# Lets compare the two test accuracies\n",
    "y_pred_grid = clfgrid.predict(X_test)\n",
    "y_pred_rnd = clfrnd.predict(X_test)\n",
    "print (\"Accuracy Grid -\",  accuracy_score_numpy(y_pred_grid))\n",
    "print (\"Accuracy RandomSearch -\",  accuracy_score_numpy(y_pred_rnd))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ensembling Techniques.\n",
    "\n",
    "Boosting is a really powerful technique. Luckily, there are many implemented in sklearn already\n",
    "* Q1] Implement Boostrapping in the traditional sense. Look at _sklearn.cross_validation.Bootstrap_\n",
    "        * Hint : [http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/modules/generated/sklearn.cross_validation.Bootstrap.html]\n",
    "        \n",
    "scikit module here : __sklearn.ensemble__\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score :  0.770642201835\n",
      "Score : 0.813455657492\n",
      "Score : 0.807339449541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "#Adaboosting\n",
    "clf = AdaBoostClassifier(n_estimators=100)\n",
    "clf.fit(x_train,y_train)\n",
    "scores = cross_val_score(clf, x_test, y_test)\n",
    "print(\"Score : \", scores.mean())\n",
    "\n",
    "#Gradient Boosting\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "clf.fit(x_train,y_train)\n",
    "print ( \"Score :\" , clf.score(x_test, y_test))\n",
    "\n",
    "#Bagging\n",
    "clf = BaggingClassifier(base_estimator=SVC(), n_estimators=10) # Lets take the best grid classifier\n",
    "clf.fit(x_train, y_train)\n",
    "print ( \"Score :\" , clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extras - AutoSklearn\n",
    "\n",
    "We hate showing this, but here goes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time limit for a single run is higher than total time limit. Capping the limit for a single run to the total time given to SMAC (299.561931)\n",
      "[WARNING] [2017-11-27 01:23:57,659:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:23:57,659:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:24:01,936:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:24:01,936:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:38,319:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:38,319:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:55,497:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:55,497:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:59,231:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:26:59,231:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:27:04,631:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:27:04,631:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:27:46,657:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n",
      "[WARNING] [2017-11-27 01:27:46,657:smac.intensification.intensification.Intensifier] Challenger was the same as the current incumbent; Skipping challenger\n"
     ]
    }
   ],
   "source": [
    "import autosklearn.classification as AutoClf\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = AutoClf.AutoSklearnClassifier(time_left_for_this_task=300) #Default is 1 hour\n",
    "clf.fit(x_train,y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "print(\"Accuracy score\", accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
