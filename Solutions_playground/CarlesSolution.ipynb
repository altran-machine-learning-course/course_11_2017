{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Techniques with classifiers\n",
    "\n",
    "In this session, we'll play around with classifiers, and techniques to optimize them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0 - Imports and load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrainData= [[-1.54609786e+00 -1.34499549e+00  2.75868709e-16  4.81287772e-01\n",
      "  -4.44999502e-01 -1.56828509e+00  1.27192065e+00 -2.40989649e+00\n",
      "  -1.90080686e+00  5.98139336e-01  7.33522906e-02]]  with shape  (654, 11)\n",
      "TrainLables= id\n",
      "277    1\n",
      "Name: survived, dtype: int64 (654,)\n",
      "TestData= [[ 0.84191642 -1.34499549 -0.8449216   0.48128777 -0.4449995   0.53185321\n",
      "  -1.30008367  0.50783544  0.59567091 -0.719428    0.07335229]]  with shape  (655, 11)\n",
      "TestLabels= id\n",
      "621   NaN\n",
      "Name: survived, dtype: float64 (654,)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression,Perceptron\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Add input as import path\n",
    "sys.path.insert(0,'../input')\n",
    "\n",
    "import joblib #or your dataset handler\n",
    "X, Y = joblib.load(\"traindata.pkl\")\n",
    "X_test, Y_test = joblib.load(\"testdata.pkl\")\n",
    "\n",
    "#Double check if all is well\n",
    "print(\"TrainData=\",X[0:1],\" with shape \", X.shape)\n",
    "print(\"TrainLables=\", Y[0:1], Y.shape) \n",
    "\n",
    "print(\"TestData=\",X_test[0:1], \" with shape \",X_test.shape)\n",
    "print(\"TestLabels=\", Y_test[0:1], Y.shape) # We don't have test labels. Should be NaNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "# Initialize different models\n",
    "# Random_state 42 https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy\n",
    "random_state=42\n",
    "multiple_classifier = []\n",
    "multiple_classifier.append(SVC(random_state=random_state))\n",
    "multiple_classifier.append(DecisionTreeClassifier(random_state=random_state))\n",
    "multiple_classifier.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),learning_rate=0.1))\n",
    "multiple_classifier.append(RandomForestClassifier(random_state=random_state))\n",
    "multiple_classifier.append(ExtraTreesClassifier(random_state=random_state))\n",
    "multiple_classifier.append(GradientBoostingClassifier(random_state=random_state))\n",
    "multiple_classifier.append(MLPClassifier(random_state=random_state))\n",
    "multiple_classifier.append(KNeighborsClassifier())\n",
    "multiple_classifier.append(LogisticRegression(random_state=random_state))\n",
    "multiple_classifier.append(LinearDiscriminantAnalysis())\n",
    "multiple_classifier.append(Perceptron())\n",
    "\n",
    "cv_results = []\n",
    "for classifier in multiple_classifier :\n",
    "    cv_results.append(cross_val_score(classifier, X, y = Y, scoring = \"accuracy\", cv = StratifiedKFold(n_splits=10), n_jobs=4))\n",
    "    classifier.fit(X,Y)\n",
    "cv_means = []\n",
    "cv_std = []\n",
    "for cv_result in cv_results:\n",
    "    cv_means.append(cv_result.mean())\n",
    "    cv_std.append(cv_result.std())\n",
    "\n",
    "cv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n",
    "\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\",\"Perceptron\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Algorithm  CrossValMeans  CrossValerrors\n",
      "0                          SVC       0.814895        0.044701\n",
      "1                 DecisionTree       0.750653        0.046052\n",
      "2                     AdaBoost       0.766014        0.027747\n",
      "3                 RandomForest       0.784406        0.049920\n",
      "4                   ExtraTrees       0.770629        0.047003\n",
      "5             GradientBoosting       0.801166        0.040181\n",
      "6      MultipleLayerPerceptron       0.808858        0.042147\n",
      "7                  KNeighboors       0.798112        0.032997\n",
      "8           LogisticRegression       0.785921        0.032841\n",
      "9   LinearDiscriminantAnalysis       0.787622        0.043737\n",
      "10                  Perceptron       0.700303        0.053873\n"
     ]
    }
   ],
   "source": [
    "print(cv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 4 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Done  40 out of  40 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features=3, max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=10,\n",
      "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=500, n_jobs=1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "random_forest: 0.8241590214067278\n"
     ]
    }
   ],
   "source": [
    "# RandomForest Hyperparameter tuning\n",
    "random_forest = RandomForestClassifier()\n",
    "kfold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "## Optimal parameters grid\n",
    "random_forest_params = {\"max_depth\": [None],\n",
    "                  \"max_features\": [3],\n",
    "                  \"min_samples_split\": [2],\n",
    "                  \"min_samples_leaf\": [10,30],\n",
    "                  \"bootstrap\": [True],\n",
    "                  \"n_estimators\" :[500,5000],\n",
    "                  \"criterion\": [\"gini\"]}\n",
    "\n",
    "\n",
    "opt_random_forest = GridSearchCV(random_forest,param_grid = random_forest_params, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose=1)\n",
    "opt_random_forest.fit(X,Y)\n",
    "opt_random_forest_best = opt_random_forest.best_estimator_\n",
    "print(opt_random_forest_best)\n",
    "# Best score\n",
    "print(\"random_forest:\", opt_random_forest.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(654, 13)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
       "       beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
       "       hidden_layer_sizes=(20, 20), learning_rate='constant',\n",
       "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
       "       nesterovs_momentum=True, power_t=0.5, random_state=None,\n",
       "       shuffle=True, solver='adam', tol=0.0001, validation_fraction=0.1,\n",
       "       verbose=False, warm_start=False)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X_TEST -> Y_TEST\n",
    "multiple_classifier.append(opt_random_forest)\n",
    "\n",
    "new_X = np.zeros((len(Y), len(multiple_classifier)))\n",
    "for classifier in multiple_classifier:\n",
    "    try:\n",
    "        new_X[:,multiple_classifier.index(classifier)] = classifier.predict_proba(X)[:,1]\n",
    "    except:\n",
    "        try:\n",
    "            new_X[:,multiple_classifier.index(classifier)] = classifier.decision_function(X)[:]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "# Scale data\n",
    "norm = Normalizer()\n",
    "norm.fit(new_X)\n",
    "X_new_normalized= norm.transform(new_X)\n",
    "\n",
    "print(X_new_normalized.shape)\n",
    "\n",
    "# Train the ensamble\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(20,20))\n",
    "mlp.fit(X_new_normalized,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1590304   0.18818362  0.18818362  0.15054689  0.18818362  0.09609291\n",
      "  0.1082718   0.18818362  0.09329373  0.10303251 -0.87153291  0.10269861\n",
      "  0.        ]\n",
      "(655, 13)\n",
      "0.7709923664122137\n"
     ]
    }
   ],
   "source": [
    "new_X_test = np.zeros((len(X_test), len(multiple_classifier)))\n",
    "for classifier in multiple_classifier:\n",
    "    try:\n",
    "        new_X_test[:,multiple_classifier.index(classifier)] = classifier.predict_proba(X_test)[:,1]\n",
    "    except:\n",
    "        try:\n",
    "            new_X_test[:,multiple_classifier.index(classifier)] = classifier.decision_function(X_test)[:]\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "# Scale data\n",
    "X_new_normalized_test = norm.transform(new_X_test)\n",
    "\n",
    "print(X_new_normalized_test[1,:])\n",
    "print(X_new_normalized_test.shape)\n",
    "\n",
    "# Train the ensamble\n",
    "\n",
    "y_new_predict = mlp.predict(X_new_normalized_test)\n",
    "# Score\n",
    "print(accuracy_score_numpy(y_new_predict))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8045801526717558\n",
      "0.7938931297709924\n",
      "0.8045801526717558\n",
      "0.8030534351145038\n"
     ]
    }
   ],
   "source": [
    "from utils import accuracy_score_numpy\n",
    "\n",
    "# Ensemble\n",
    "Y_test = voting.predict(X_test)\n",
    "print(accuracy_score_numpy(Y_test))\n",
    "\n",
    "# opt_svc\n",
    "Y_test = opt_svc.predict(X_test)\n",
    "print(accuracy_score_numpy(Y_test))\n",
    "\n",
    "# opt_random_forest\n",
    "Y_test = opt_random_forest.predict(X_test)\n",
    "print(accuracy_score_numpy(Y_test))\n",
    "\n",
    "# opt_gradient_boost\n",
    "Y_test = opt_gradient_boost.predict(X_test)\n",
    "print(accuracy_score_numpy(Y_test))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
