{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d0fff099-32b4-4d5f-94f2-dc957e8d7b58",
    "_uuid": "0a7336426503aa654b923d813483e106306716f9"
   },
   "source": [
    "# Week 1 - The What Why and when of Machine Learning\n",
    "Main idea is to follow that notebook in order to have an initial aproach -  this doesn't means that you only have to answer to the questions or implement code. Feel free to add all the interesting content that you find and take conclusions from the data. Please just use this as a template"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Library import and list input files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "a4c3c51c-f465-4cfc-913a-ee1225e57724",
    "_uuid": "15592da27c5f06e6ab4fb0a293a851cc8489453b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sys\n",
    "\n",
    "\n",
    "#Ignore Warnings - save some confusion\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Pandas more columns\n",
    "pd.options.display.max_columns = None\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Add input as import path\n",
    "sys.path.insert(0,'../input')\n",
    "\n",
    "# Plot style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Import the data from the dataset\n",
    "train_data = pd.read_csv('../input/train.csv',index_col='id')\n",
    "test_data = pd.read_csv('../input/test.csv',index_col='id')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d96be262-1ded-4cd3-9193-03d7dc0b86bb",
    "_uuid": "823783aab011b613db23022d071be8684faa60e0"
   },
   "source": [
    "# 1. Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "### 1.1 Data engineering.\n",
    "Objectives:\n",
    "* Create three new features using the actual features.\n",
    "* Create plots like in the first session. (Categorical / Numerical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' Mrs', ' Miss', ' Mr', ' Master', ' Jonkheer', ' Dr', ' Mme',\n",
       "       ' Ms', ' Major', ' Rev', ' the Countess', ' Dona'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Todo 1 : Create a new feature that describes if people travels alone or not.\n",
    "#dataset['TravelAlone'] = ...\n",
    "\n",
    "# Todo 2 : Create a new feature that represents the FamilySize - for each passenger a new column with a number 1..n\n",
    "#dataset['FamilySize'] = ...\n",
    "\n",
    "# Todo 3 : Create a new feature based in the Title name for example Sr, Mrs, Rev.\n",
    "# Clue 1: As we can see different classes of title name exist. Decide how to group and decide \n",
    "train_data['name'].str.split(\",\",expand=True)[1].str.split(\".\",expand=True)[0].unique()\n",
    "# Your work\n",
    "# dataset['RareName'] = ....\n",
    "\n",
    "# Optional : Plot the data in plots like in the previous training notebook (Categorical / Numerical)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "### 1.2 Data correlation.\n",
    "Objectives:\n",
    "* Create a heatmap that show the data correlation\n",
    "* Understand the data correlation\n",
    "\n",
    "Questions:\n",
    "* Q1: Which are the top three pairs of data with more correlation? (ToDo 1..2 and 3..4 ) \n",
    "* Q2: Is the sex of the passengers related with survival?\n",
    "* Q3: If two features have a lot of correlation we want to preserve both or only use one?\n",
    "* Q4: Are the previous new created features related with the survivance in the accident? Choose the one that you think that is the best.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "ac3cbd95-2242-4251-90ca-dd65c34e17b3",
    "_uuid": "6745659346d0b3d3a9b6424ce123307f20be533f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          pclass       age\n",
      "pclass  1.000000 -0.417239\n",
      "age    -0.417239  1.000000\n",
      "Data Correlation for the features pclass/age\n",
      "Q1: This is my response for the first question!\n"
     ]
    }
   ],
   "source": [
    "print (train_data[[\"pclass\",\"age\"]].corr())\n",
    "\n",
    "# PreTodo. Copying train_data into another variable. Some examples\n",
    "sample_data = train_data.copy()\n",
    "sex_types = {\"male\":1, \"female\":0}\n",
    "sample_data[\"sex\"] = sample_data[\"sex\"].apply(sex_types.get) # Converting into numerical label\n",
    "#Work with your train data , the same way we work with sample_data\n",
    "\n",
    "\n",
    "\n",
    "# ToDO 1: Compute the correlation for all the features ( Remember that the survived column is our label )\n",
    "print(\"Data Correlation for the features pclass/age\")\n",
    "data_correlation = train_data.corr()\n",
    "# ToDo 2: Plot the data in a fancy plot like the sns.heatmap\n",
    "#sns.heatmap(....)\n",
    "# ToDo 3: Compute the correlation between the sex feature and survived label \n",
    "#sex_survived_correlation = ...\n",
    "# ToDo 4: Plot the data in a fancy plot like the sns.heatmap\n",
    "# sns.heatmap(....)\n",
    "# Todo 3: Questions\n",
    "print(\"Q1: This is my response for the first question!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b764727f-3455-4d7b-8eac-101b52644cf4",
    "_uuid": "028fec15cf0c5f625c122db5e726c9e6f1073905"
   },
   "source": [
    "# 2. Data visualization.\n",
    "Objectives:\n",
    "* Use PCA to reduce the dimensions of our data\n",
    "* Analyze the separability of our data\n",
    "\n",
    "Questions:\n",
    "* Q1: Is the data separable?\n",
    "* Q2: You think that we need to reduce the dimensionality of our data?\n",
    "* Q3: Find whitch infromation returns the explained_variance_ratio_ method\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'male'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-1e607fac2b7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;31m# 3. Scale the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[0mscaler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMinMaxScaler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[0mtrain_data_scaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data_subset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data_subset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;31m#After\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m             \u001b[1;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 518\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    519\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[1;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    306\u001b[0m         \u001b[1;31m# Reset internal state before fitting\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpartial_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py\u001b[0m in \u001b[0;36mpartial_fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    333\u001b[0m         X = check_array(X, copy=self.copy, warn_on_dtype=True,\n\u001b[1;32m--> 334\u001b[1;33m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[0;32m    335\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    336\u001b[0m         \u001b[0mdata_min\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                       force_all_finite)\n\u001b[0;32m    401\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m         \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    403\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'male'"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA as sklearnPCA\n",
    "# \n",
    "\"\"\"\n",
    "First of all we need to scale our data we have different features measured in different \"units\" what we need is to normali\n",
    "this features to be in the same range, the easiest way to do this is with the MinMaxScaler\n",
    "\"\"\"\n",
    "\n",
    "# 1. Select a subset of data\n",
    "#train_data_subset = train_data[[\"age\", \"sex\", \"pclass\", \"embarked\", \"TravelAlone\"]]\n",
    "train_data_subset = train_data[[\"age\", \"pclass\", \"sibsp\", \"embarked\", \"sex\"]]\n",
    "\n",
    "# Todo 1: Convert the categorical labels to numerical for example Sex can be converted to a boolean\n",
    "# train_data_subset[\"sex\"] = ...\n",
    "# train_data_subset[\"embarked\"] = ....\n",
    "\n",
    "# 2. Remove NA with 0 probably exist a better way :)\n",
    "train_data_subset.fillna(0, inplace=True)\n",
    "\n",
    "# 3. Scale the data\n",
    "scaler = MinMaxScaler()\n",
    "train_data_scaled = pd.DataFrame(scaler.fit_transform(train_data_subset), columns=train_data_subset.columns)\n",
    "\n",
    "#After\n",
    "print(\"Before scale:\")\n",
    "print(train_data_subset[0:3])\n",
    "print(\"\\n\")\n",
    "#Before\n",
    "print(\"After scale:\")\n",
    "print(train_data_scaled[0:3])\n",
    "print(\"\\n\")\n",
    "\n",
    "# 4. Compute PCA\n",
    "pca = sklearnPCA(n_components=2) #2-dimensional PCA\n",
    "transformed = pd.DataFrame(pca.fit_transform(train_data_scaled))\n",
    "train_data.index = range(len(train_data.index))\n",
    "\n",
    "# 5. Plot the data with the reduced dimentions\n",
    "plt.scatter(transformed[train_data[\"survived\"]==0][0], transformed[train_data[\"survived\"]==0][1], label='Died', c='red')\n",
    "plt.scatter(transformed[train_data[\"survived\"]==1][1], transformed[train_data[\"survived\"]==1][1], label='Survived', c='blue')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 6. Interesting methods\n",
    "print(pca.explained_variance_ratio_ )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
